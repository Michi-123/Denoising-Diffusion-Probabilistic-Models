# -*- coding: utf-8 -*-
"""DataLoader2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ofcvV2khO7B-hzNxlQON1cWXX2H1BecO
"""

import torch
from torch.utils.data import Dataset, DataLoader

# @title All in One
class TranslationDataset(Dataset):
    def __init__(self, pairs, max_sequence_length):
        self.pairs = pairs
        self.max_sequence_length = max_sequence_length
        self.source_vocab, self.target_vocab = self.build_vocab()

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        source_text, target_text = self.pairs[idx]

        source_tokens = self.tokenize(source_text)
        target_tokens = self.tokenize(target_text)

        padded_source_tokens = self.pad_sequence(source_tokens, self.max_sequence_length)
        padded_target_tokens = self.pad_sequence(target_tokens, self.max_sequence_length)

        source_indices = self.tokens_to_indices(padded_source_tokens, self.source_vocab)
        target_indices = self.tokens_to_indices(padded_target_tokens, self.target_vocab)

        return {
            'source_indices': torch.tensor(source_indices),
            'target_indices': torch.tensor(target_indices),
        }

    def build_vocab(self):
        source_texts, target_texts = zip(*self.pairs)

        source_tokens = [token for text in source_texts for token in self.tokenize(text)]
        target_tokens = [token for text in target_texts for token in self.tokenize(text)]

        source_unique_tokens = set(source_tokens + ['[PAD]'])  # [PAD] を追加
        target_unique_tokens = set(target_tokens + ['[PAD]'])  # [PAD] を追加

        source_vocab = {token: idx for idx, token in enumerate(source_unique_tokens)}
        target_vocab = {token: idx for idx, token in enumerate(target_unique_tokens)}

        return source_vocab, target_vocab

    def tokenize(self, text):
        # Simple tokenization, split by spaces
        return text.split()

    def pad_sequence(self, tokens, max_length):
        if len(tokens) < max_length:
            padding = ['[PAD]'] * (max_length - len(tokens))
            tokens += padding
        else:
            tokens = tokens[:max_length]
        return tokens

    def tokens_to_indices(self, tokens, vocab):
        return [vocab[token] for token in tokens]

pairs = [
    ["Hello how are you?", "こんにち は 、 お 元気 です か ？"],
    ["This is a translation example.", "これ は 翻訳 の 例 です 。"],
    ["He is", "彼 です"],
]

max_sequence_length = 10
translation_dataset = TranslationDataset(pairs, max_sequence_length)
translation_dataloader = DataLoader(translation_dataset, batch_size=2, shuffle=False)

# Iterate over the dataloader
for batch in translation_dataloader:

    source_indices = batch['source_indices']
    target_indices = batch['target_indices']

    # Your training code here
    print("Source Indices:", source_indices)
    print("Target Indices:", target_indices)
    print("---")

